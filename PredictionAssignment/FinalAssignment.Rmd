---
title: "Activity Quality Prediction"
author: "Todd Kemmerling"
date: "September 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
```

## Loading the data and test sets

```{r loading}
df <- read.csv('pml-training.csv')
tt <- read.csv('pml-testing.csv')
ncolumns <- ncol(df)
```

The initial number of columns in the data frame is `r ncolumns` columns.

## Reducing the number of features

The number of features/columns was reduced by eliminating empty columns and columns that have large numbers of missing values. This reduced the data set to the following columns.

```{r allcolumns}
allUsedColumns <- c("user_name","roll_belt","pitch_belt","yaw_belt","total_accel_belt",
                    "gyros_belt_x","gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y",
                    "accel_belt_z", "magnet_belt_x","magnet_belt_y","magnet_belt_z","roll_arm",
                    "pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", "gyros_arm_y",
                    "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x",
                    "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", 
                    "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z",
                    "accel_dumbbell_x", "accel_dumbbell_y","accel_dumbbell_z","magnet_dumbbell_x",
                    "magnet_dumbbell_y","magnet_dumbbell_z", "roll_forearm","pitch_forearm","yaw_forearm",
                    "total_accel_forearm", "gyros_forearm_x","gyros_forearm_y", "gyros_forearm_z", 
                    "accel_forearm_x", "accel_forearm_y", "accel_forearm_z","magnet_forearm_x", 
                    "magnet_forearm_y", "magnet_forearm_z","classe")
ncolumns <- length(allUsedColumns)
df <- df[,allUsedColumns]
```

This using this simple method the number of columns was reduced to `r ncolumns`.

## Initial Setup and Model Selection

The setup for model training is as follows.

```{r trainingsetup}
labelName <- 'classe'
predictors <- names(df)[names(df) != labelName]
set.seed(819)
```

For the inital algorithm I chose 'Random Forest'. The main reason is that it is supposed to be good and figuring out the importance of features. Given that the
dataset has only been reduced to 57 feature columns at this point. It seemed like a good idea.

The training is setup for cross validation with 5 kfolds.

```{r modelgen}
model <- train(classe~., data=df, trControl=trainControl(method="cv",number=5), method="rf")
```


```
print(model$finalModel)

randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 29

        OOB estimate of  error rate: 0.42%
Confusion matrix:
     A    B    C    D    E class.error
A 5574    3    1    0    2 0.001075269
B   19 3773    5    0    0 0.006320780
C    0    9 3402   11    0 0.005844535
D    0    0   21 3193    2 0.007151741
E    0    1    3    6 3597 0.002772387
```

```{r plotting}
plot(model$finalModel, main='Plot of model trees vs error')
```

```{r testset}
predictedValues <- predict(model, tt[,predictors])
predictedValues
```

## Summary

The data above shows an estimated error rate of 0.42% after. The predicted values shown above were used for the quiz and the resulting grade is/was 100%. It is believed that
the number of feature could be reduced further, but that work as not been done at this time.















